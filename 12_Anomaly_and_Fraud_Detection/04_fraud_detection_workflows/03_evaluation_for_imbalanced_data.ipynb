{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc98e80e-0505-4de9-89e0-892c063e9406",
   "metadata": {},
   "source": [
    "# 03_evaluation_for_imbalanced_data.ipynb\n",
    "\n",
    "## Objective\n",
    "\n",
    "Evaluate fraud detection models under **extreme class imbalance**, focusing on business-aligned metrics instead of misleading accuracy.\n",
    "\n",
    "\n",
    "## Why Standard Metrics Fail\n",
    "\n",
    "* Accuracy hides fraud miss-rate\n",
    "* ROC-AUC can look good while fraud recall is poor\n",
    "* Class imbalance distorts threshold-based decisions\n",
    "\n",
    "\n",
    "\n",
    "## Key Evaluation Metrics\n",
    "\n",
    "### 1. Precision, Recall, F1\n",
    "\n",
    "* Precision: cost of false positives\n",
    "* Recall: cost of missed fraud\n",
    "* F1: balance signal (use cautiously)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac616aa-608d-45be-9f46-26ae607481a6",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d9712-5ba6-461c-ad61-26ecc8eae24b",
   "metadata": {},
   "source": [
    "### 2. Precision–Recall Curve\n",
    "\n",
    "Preferred for rare-event detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c150a6-27c9-4ff1-ba59-cbab321a8fde",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138c6dd-e429-4f4f-af38-6a637862f3f3",
   "metadata": {},
   "source": [
    "* PR-AUC reflects fraud detection quality\n",
    "* Visualizes trade-off between investigation load and fraud capture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e1bbd-35f9-4762-b4ec-dd2976abe2fb",
   "metadata": {},
   "source": [
    "### 3. ROC Curve (Secondary)\n",
    "\n",
    "Useful for model comparison, **not deployment**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfa3b1-6ee3-477d-a4f1-bb8d4e711344",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import roc_auc_score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0315e-93b5-4202-b95a-6afcfdc9fdff",
   "metadata": {},
   "source": [
    "## Threshold Analysis\n",
    "\n",
    "### Why 0.5 Is Wrong\n",
    "\n",
    "* Fraud models output risk, not truth\n",
    "* Threshold must reflect business constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f153f6-c80f-4f94-b88f-f0804330bc8f",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cff44-ef27-4bad-9bba-670536e52504",
   "metadata": {},
   "source": [
    "Analyze:\n",
    "\n",
    "* Recall vs threshold\n",
    "* Precision vs threshold\n",
    "* Fraud caught vs alerts generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0d1da-2a0a-4691-99ef-8dabe7d466b3",
   "metadata": {},
   "source": [
    "## Cost-Sensitive Evaluation\n",
    "\n",
    "Define costs:\n",
    "\n",
    "* False Negative (missed fraud)\n",
    "* False Positive (manual review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc059f1-8507-40e6-9343-6cc7b8a7e9a7",
   "metadata": {},
   "source": [
    "# example cost function\n",
    "```python\n",
    "expected_cost = fn * cost_fn + fp * cost_fp\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15879cf4-6168-4581-a17d-1b82c184c352",
   "metadata": {},
   "source": [
    "Select threshold minimizing expected loss.\n",
    "\n",
    "\n",
    "\n",
    "## Confusion Matrix at Multiple Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f58051-ce57-4a90-ad5e-1518a22a4545",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c6d1e-9aa5-414b-b501-30ec364b6e8d",
   "metadata": {},
   "source": [
    "Compare operational impact at different cutoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da12dc-4d93-4afe-871e-e84eeb7c5359",
   "metadata": {},
   "source": [
    "## Model Comparison Summary Table\n",
    "\n",
    "Track:\n",
    "\n",
    "* PR-AUC\n",
    "* Recall @ fixed FP rate\n",
    "* Expected cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a81d2-dbde-4e7f-bf26-11ac0d1168ff",
   "metadata": {},
   "source": [
    "## Production Checklist\n",
    "\n",
    "* Always evaluate on **time-based split**\n",
    "* Lock threshold before deployment\n",
    "* Monitor precision drift post-release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df844ea-32af-4ae1-a2d0-dece1f229175",
   "metadata": {},
   "source": [
    "## Output of This Notebook\n",
    "\n",
    "* Selected operating threshold\n",
    "* Approved evaluation metrics\n",
    "* Go/No-Go decision for deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Next Notebook\n",
    "\n",
    "- [04_pipeline_integration_for_imbalanced.ipynb](04_pipeline_integration_for_imbalanced.ipynb)\n",
    "\n",
    "> End-to-end pipeline: preprocessing → features → model → threshold → inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630b6a5-c4b8-412f-aa28-823e4ace8edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d68838-5b43-4927-aed3-353916a4721f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
