{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53b3a50-cd3a-4af7-aef9-2aca66b3fc29",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers: Stability, Efficiency, and Performance\n",
    "## Objective\n",
    "\n",
    "Refine transformer fine-tuning by introducing advanced training techniques that improve:\n",
    "\n",
    "- Generalization\n",
    "- Training stability\n",
    "- Resource efficiency\n",
    "\n",
    "> This notebook moves from “it works” to “it works reliably in production”.\n",
    "\n",
    "## Why Fine-Tuning Needs Care\n",
    "\n",
    "Naive fine-tuning often results in:\n",
    "\n",
    "- Overfitting on small datasets\n",
    "- Unstable validation metrics\n",
    "- Excessive GPU memory usage\n",
    "- Non-reproducible results\n",
    "Proper tuning addresses these risks systematically.\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89b936b-0401-48f0-82ad-5f5e9f87e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4846653-05db-470d-b0f2-5ac0a522463f",
   "metadata": {},
   "source": [
    "# Reproducibility Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f6c63e-160d-43e9-a654-616b01a5a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED =  2010\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5299c8-2568-4b24-bf3f-d8b20c266402",
   "metadata": {},
   "source": [
    "# Dataset \n",
    "- Same Structure as Previous Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590916f7-b91c-4caa-8bba-8acf3a09bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"text\": [\n",
    "        \"This model works very well\",\n",
    "        \"Excellent performance and stability\",\n",
    "        \"Terrible results and poor accuracy\",\n",
    "        \"Bad predictions and unreliable output\",\n",
    "        \"Robust and interpretable system\",\n",
    "        \"Awful behavior and weak model\",\n",
    "        \"Strong and consistent performance\",\n",
    "        \"Poor generalization and bad results\"\n",
    "    ],\n",
    "    \"label\": [1, 1, 0, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e309b0-f5db-458a-8c70-dcfd16c3dd47",
   "metadata": {},
   "source": [
    "# Train / Validation Split (Leakage-Safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938a1250-6ee8-450a-9c2d-b50c052ac925",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.25,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4332d8-9f9f-4fa4-a456-30eaa15b8e41",
   "metadata": {},
   "source": [
    "# Hugging Face Dataset Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b21cb-077d-4985-91c0-dd7510907d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab8da9a-9f84-40c7-8faf-a54c9a76df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f59c0c-ce24-4cef-96f2-f7e1131e84aa",
   "metadata": {},
   "source": [
    "# Model and Tokenizer\n",
    "## Choosing a Lighter Model (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e2cc00-b106-4857-9b0f-f68f330f3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59780693-6872-48f1-b5f4-43855dea3e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccf0cd321db4faeb645ab5a7153fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantu\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pantu\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de0e7922a214026a0e6e4c1a2c40f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d4f73c93f34636b80b3b5f100ed8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c841ec554a4c56b0d29be7f8fbfe35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0330f6507e4440da9338a39fe8aec35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9357462-b4c8-4821-add4-db9b75b96a80",
   "metadata": {},
   "source": [
    "## Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08ca8298-143c-43cd-ac8f-f3151d151d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531b6fb-a272-48e8-a56f-ec9086862c33",
   "metadata": {},
   "source": [
    "## Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d74e1bf3-3ed6-4cb5-9e39-6d35daed7c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671906bdb8204fbab1db537ba79e8d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f214df9b09b40c9850fd7cf5da0267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds.map(tokenize_batch, batched=True)\n",
    "val_ds = val_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "columns = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_ds.set_format(type=\"torch\", columns=columns)\n",
    "val_ds.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57beb79-e335-4a85-a30b-c95096d8543e",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "376dba4f-6e92-4adc-863c-d8caa6d85bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1dbd6-869e-409f-82ac-0e6988730324",
   "metadata": {},
   "source": [
    "## Advanced Training Techniques\n",
    "### 1. Early Stopping\n",
    "\n",
    "Stops training when validation performance plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a491d767-f2db-4357-b656-3b13eeb704ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1560ed-0f85-4291-bd41-fdd1ca686148",
   "metadata": {},
   "source": [
    "### 2. Gradient Accumulation\n",
    "\n",
    "Simulates larger batch sizes with limited memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f782f13d-df57-4d52-82de-156d75f6aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc0c27-617d-4b07-bbd6-1468676fe9c3",
   "metadata": {},
   "source": [
    "### 3. Learning Rate Scheduling + Warmup\n",
    "\n",
    "Improves convergence stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9faddd45-f55f-49a3-b878-0e010681ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44d146d-0566-408d-949d-d7f8534f6ac8",
   "metadata": {},
   "source": [
    "## Training Arguments (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f48bb8cc-a44e-4b3d-96ad-4aa0596c084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./transformer_finetune\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a964d8-c8c5-431a-9346-27af2c9a8338",
   "metadata": {},
   "source": [
    "## Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cda2415d-faaa-4b9d-a576-d5c3692de7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantu\\AppData\\Local\\Temp\\ipykernel_27472\\2164134656.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6cf71a-acdc-45e7-b9de-d6835c2c651d",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58626260-2590-4a65-9755-4c67463f5a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/10 00:05 < 00:38, 0.18 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687510</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.683108</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.677833</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\pantu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.6850541432698568, metrics={'train_runtime': 6.6977, 'train_samples_per_second': 8.958, 'train_steps_per_second': 1.493, 'total_flos': 596103293952.0, 'train_loss': 0.6850541432698568, 'epoch': 3.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0dcc1-7606-4f07-ba3b-975c7f7586d8",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fea51884-7efe-41a3-8c01-0c576168c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pantu\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6875101327896118,\n",
       " 'eval_model_preparation_time': 0.002,\n",
       " 'eval_accuracy': 0.5,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_runtime': 0.09,\n",
       " 'eval_samples_per_second': 22.213,\n",
       " 'eval_steps_per_second': 11.107,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2525a99-7d9a-4f31-84ff-5abc5c0194bd",
   "metadata": {},
   "source": [
    "\n",
    "# Hyperparameters That Matter Most\n",
    "\n",
    "| Parameter     | Impact |\n",
    "| ------------- | ------ |\n",
    "| Learning rate | ⭐⭐⭐⭐⭐  |\n",
    "| Batch size    | ⭐⭐⭐⭐   |\n",
    "| Epochs        | ⭐⭐⭐    |\n",
    "| Warmup        | ⭐⭐⭐    |\n",
    "| Weight decay  | ⭐⭐     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Practical Fine-Tuning Guidelines\n",
    "\n",
    "- Prefer smaller models unless necessary\n",
    "\n",
    "- Use early stopping by default\n",
    "\n",
    "- Avoid long sequences unless required\n",
    "\n",
    "- Monitor F1, not just accuracy\n",
    "\n",
    "- Log everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faca309-c8e5-43fd-a813-6ec90fb199be",
   "metadata": {},
   "source": [
    "# Common Fine-Tuning Mistakes\n",
    "\n",
    "- `[neg] -` Over-training tiny datasets\n",
    "- `[neg] -` Using large models blindly\n",
    "- `[neg] -` Ignoring validation instability\n",
    "- `[neg] -` Treating one run as conclusive\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "- Fine-tuning is optimization, not brute force\n",
    "Stability techniques often matter more than - architecture\n",
    "- Early stopping saves both time and performance\n",
    "- Production NLP requires disciplined experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f3fd3-75ed-4f0b-b623-9a72eaec8127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5b113-da66-442a-87b6-8bb2722ea03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
