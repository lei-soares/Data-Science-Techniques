{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12924885-0c33-4326-b40b-2b852d571c4e",
   "metadata": {},
   "source": [
    "# Feature Extraction: Bag-of-Words and TF-IDF\n",
    "## Objective\n",
    "\n",
    "Transform normalized textual data into numerical feature representations suitable for:\n",
    "\n",
    "- Classical machine learning models\n",
    "\n",
    "- Baseline NLP systems\n",
    "\n",
    "- Interpretable text analytics\n",
    "\n",
    "> This notebook focuses on sparse, high-dimensional representations that trade semantic richness for transparency and control.\n",
    "\n",
    "## Why Feature Extraction Matters\n",
    "\n",
    "Machine learning models do not understand text — they understand numbers.\n",
    "\n",
    "Poor feature extraction leads to:\n",
    "\n",
    "- Sparse but meaningless vectors\n",
    "\n",
    "- Overfitting on rare terms\n",
    "\n",
    "- Unstable model coefficients\n",
    "\n",
    "- Leakage via improper fitting\n",
    "\n",
    "BoW and TF-IDF remain:\n",
    "\n",
    "- Strong baselines\n",
    "\n",
    "- Highly interpretable\n",
    "\n",
    "- Computationally efficient\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4720801d-58dc-415e-ad17-f8d8089ae3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3dc184-f86c-4dee-9db9-aa64dfe9bf18",
   "metadata": {},
   "source": [
    "## Example Corpus (From Previous Notebooks)\n",
    "\n",
    "We assume text has been:\n",
    "\n",
    "- Cleaned\n",
    "\n",
    "- Tokenized\n",
    "\n",
    "- Normalized (stopwords + lemmatization)\n",
    "\n",
    "For vectorizers, we rejoin tokens into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f90d4a-a986-4339-a958-6007f2b593f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_normalized</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[amazing, visit]</td>\n",
       "      <td>amazing visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, hard]</td>\n",
       "      <td>nlp hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, error, silent, model, failure]</td>\n",
       "      <td>tokenization error silent model failure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "      <td>clean text better model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_normalized  \\\n",
       "0                               [amazing, visit]   \n",
       "1                                    [nlp, hard]   \n",
       "2  [tokenization, error, silent, model, failure]   \n",
       "3                   [clean, text, better, model]   \n",
       "\n",
       "                           text_normalized  \n",
       "0                            amazing visit  \n",
       "1                                 nlp hard  \n",
       "2  tokenization error silent model failure  \n",
       "3                  clean text better model  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"tokens_normalized\": [\n",
    "        [\"amazing\", \"visit\"],\n",
    "        [\"nlp\", \"hard\"],\n",
    "        [\"tokenization\", \"error\", \"silent\", \"model\", \"failure\"],\n",
    "        [\"clean\", \"text\", \"better\", \"model\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df[\"text_normalized\"] = df[\"tokens_normalized\"].apply(lambda x: \" \".join(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ece0b-574b-482e-ac4d-98241d170631",
   "metadata": {},
   "source": [
    "# Bag-of-Words (BoW)\n",
    "Concept\n",
    "\n",
    "BoW represents text as:\n",
    "\n",
    "- Token counts\n",
    "- \n",
    "Order-agnostic\n",
    "\n",
    "- High-dimensional sparse vectors\n",
    "\n",
    "## Fit CountVectorizer\n",
    "\n",
    " __Important:__ Always fit only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0c8d38-8baf-4c50-a8ad-98d8d963d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "X_bow = count_vectorizer.fit_transform(df[\"text_normalized\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78243b35-2bcb-44ea-8714-91cab0e86739",
   "metadata": {},
   "source": [
    "### Inspect Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf132bf6-1c24-474b-b386-27b30fbc5f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amazing', 'better', 'clean', 'error', 'failure', 'hard', 'model',\n",
       "       'nlp', 'silent', 'text', 'tokenization', 'visit'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = count_vectorizer.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558fac8-661c-418f-8b40-863d7b4f8a3c",
   "metadata": {},
   "source": [
    "### Dense View (For Inspection Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c2012c-6925-4594-bcd4-4f74257b4ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>better</th>\n",
       "      <th>clean</th>\n",
       "      <th>error</th>\n",
       "      <th>failure</th>\n",
       "      <th>hard</th>\n",
       "      <th>model</th>\n",
       "      <th>nlp</th>\n",
       "      <th>silent</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  better  clean  error  failure  hard  model  nlp  silent  text  \\\n",
       "0        1       0      0      0        0     0      0    0       0     0   \n",
       "1        0       0      0      0        0     1      0    1       0     0   \n",
       "2        0       0      0      1        1     0      1    0       1     0   \n",
       "3        0       1      1      0        0     0      1    0       0     1   \n",
       "\n",
       "   tokenization  visit  \n",
       "0             0      1  \n",
       "1             0      0  \n",
       "2             1      0  \n",
       "3             0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(\n",
    "    X_bow.toarray(),\n",
    "    columns=vocab\n",
    ")\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a1b36-14c4-4fe8-932d-afff1fd2d14e",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "Concept\n",
    "\n",
    "TF-IDF weights tokens by:\n",
    "\n",
    "- Term frequency (TF)\n",
    "\n",
    "- Inverse document frequency (IDF)\n",
    "\n",
    "This reduces the impact of ubiquitous terms.\n",
    "\n",
    "### Fit TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd1ab2f-71e2-45c1-a2f0-eb2e326e6af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"text_normalized\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc97ff55-55cd-48a8-8f26-5162a5246978",
   "metadata": {},
   "source": [
    "### Inspect Vocabulary and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d47b204-5923-444d-a8f8-9da85690c884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>better</th>\n",
       "      <th>clean</th>\n",
       "      <th>error</th>\n",
       "      <th>failure</th>\n",
       "      <th>hard</th>\n",
       "      <th>model</th>\n",
       "      <th>nlp</th>\n",
       "      <th>silent</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>visit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing    better     clean     error   failure      hard     model  \\\n",
       "0  0.707107  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.465162  0.465162  0.000000  0.366739   \n",
       "3  0.000000  0.525473  0.525473  0.000000  0.000000  0.000000  0.414289   \n",
       "\n",
       "        nlp    silent      text  tokenization     visit  \n",
       "0  0.000000  0.000000  0.000000      0.000000  0.707107  \n",
       "1  0.707107  0.000000  0.000000      0.000000  0.000000  \n",
       "2  0.000000  0.465162  0.000000      0.465162  0.000000  \n",
       "3  0.000000  0.000000  0.525473      0.000000  0.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray(),\n",
    "    columns=tfidf_vocab\n",
    ")\n",
    "\n",
    "tfidf_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1ffb5-85fd-4937-ab3f-79946a1ac575",
   "metadata": {},
   "source": [
    "## BoW vs TF-IDF\n",
    "\n",
    "\n",
    "| Aspect           | BoW        | TF-IDF             |\n",
    "| ---------------- | ---------- | ------------------ |\n",
    "| Simplicity       | Very high  | High               |\n",
    "| Interpretability | Excellent  | Very good          |\n",
    "| Term weighting   | Raw counts | Frequency + rarity |\n",
    "| Default baseline | ✅          | ✅                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40afdc3-c98b-40ec-b7b6-db5191e96992",
   "metadata": {},
   "source": [
    "# Controlling Sparsity and Noise\n",
    "### Limit Vocabulary Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b03f2ccb-794a-43f7-8613-4ab03fbb6eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amazing', 'better', 'clean', 'error', 'model'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_limited = TfidfVectorizer(\n",
    "    max_features=5\n",
    ")\n",
    "\n",
    "X_limited = tfidf_limited.fit_transform(df[\"text_normalized\"])\n",
    "tfidf_limited.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87528b0d-fc16-4de9-8c51-f81a40a99887",
   "metadata": {},
   "source": [
    "## Remove Rare and Frequent Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35a358a9-e71f-4caf-9cfa-49dc33046245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['model'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pruned = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.9\n",
    ")\n",
    "\n",
    "X_pruned = tfidf_pruned.fit_transform(df[\"text_normalized\"])\n",
    "tfidf_pruned.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8429af-078d-4255-a44d-2d65c66bc1da",
   "metadata": {},
   "source": [
    "# N-grams (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0550a65-0746-4866-b25b-a6644e3b5421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['amazing', 'amazing visit', 'better', 'better model', 'clean',\n",
       "       'clean text', 'error', 'error silent', 'failure', 'hard'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ngrams = TfidfVectorizer(\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "X_ngrams = tfidf_ngrams.fit_transform(df[\"text_normalized\"])\n",
    "tfidf_ngrams.get_feature_names_out()[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38051e-69a2-4fcf-83cb-26689a8972ec",
   "metadata": {},
   "source": [
    "# Pipeline-Safe Design Pattern\n",
    "\n",
    "Vectorization must be:\n",
    "\n",
    "- Fitted on training data\n",
    "\n",
    "- Reused unchanged for validation/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a8e71b6-4932-48f4-9fb1-efb2c04be409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_vectorizer():\n",
    "    return TfidfVectorizer(\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb54956-b168-4313-895c-8c15ed16869c",
   "metadata": {},
   "source": [
    "# Common Feature Extraction Mistakes\n",
    "\n",
    "- ❌ Fitting vectorizers on full datasets\n",
    "- ❌ Changing vocabulary across experiments\n",
    "- ❌ Ignoring sparsity when choosing models\n",
    "- ❌ Treating TF-IDF as semantic embeddings\n",
    "\n",
    "# When NOT to Use BoW / TF-IDF\n",
    "\n",
    "Avoid when:\n",
    "\n",
    "- Long-range context matters\n",
    "\n",
    "- Semantic similarity is required\n",
    "\n",
    "- Cross-language generalization is needed\n",
    "\n",
    "➡ Use embeddings or transformers instead.\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "- BoW and TF-IDF are strong, interpretable baselines\n",
    "\n",
    "- Sparsity control is essential\n",
    "\n",
    "- Vectorizers must be pipeline-encapsulated\n",
    "\n",
    "- Always treat vectorizers as fitted objects\n",
    "\n",
    "# Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb3263-4b3c-45b3-945e-d3af0b3572bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a8388-c422-4400-a29e-fb0a1d00ee6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5be0d-ec71-41d1-b409-4a3c63c47927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4af3f-19a3-4064-9b75-2aed831e51a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
