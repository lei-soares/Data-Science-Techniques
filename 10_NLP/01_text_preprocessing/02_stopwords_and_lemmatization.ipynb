{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5590852-c413-4db4-824a-53c0814d8470",
   "metadata": {},
   "source": [
    "# Stopwords and Lemmatization\n",
    "## Objective\n",
    "\n",
    "Normalize tokenized text by:\n",
    "\n",
    "- Removing non-informative words\n",
    "\n",
    "- Reducing tokens to their canonical forms\n",
    "\n",
    "- The goal is signal preservation, not aggressive compression.\n",
    "\n",
    "## Why This Step Matters\n",
    "\n",
    "Without normalization:\n",
    "\n",
    "- Vocabulary size explodes\n",
    "\n",
    "- Models overfit on morphological noise\n",
    "\n",
    "- Feature importance becomes fragmented\n",
    "\n",
    "- Interpretability degrades\n",
    "\n",
    "With over-normalization:\n",
    "\n",
    "- Semantic meaning is lost\n",
    "\n",
    "- Sentiment and intent can be distorted\n",
    "\n",
    "This notebook demonstrates controlled normalization.\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8d5c71-eec0-4eea-9c15-3b782c824752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pantu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pantu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pantu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pantu\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c963c221-1422-42ce-8f5d-e5ac2fd60e53",
   "metadata": {},
   "source": [
    "## Example Tokens (From Previous Notebook)\n",
    "\n",
    "We assume tokens already exist from 01_basic_cleaning_and_tokenization.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcc0c90-848e-4c2f-ba66-991859315a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, is, amazing, visit, now]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, is, hard, or, is, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokens\n",
       "0                  [this, is, amazing, visit, now]\n",
       "1                      [nlp, is, hard, or, is, it]\n",
       "2  [tokenization, errors, silent, model, failures]\n",
       "3                    [clean, text, better, models]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"tokens\": [\n",
    "        [\"this\", \"is\", \"amazing\", \"visit\", \"now\"],\n",
    "        [\"nlp\", \"is\", \"hard\", \"or\", \"is\", \"it\"],\n",
    "        [\"tokenization\", \"errors\", \"silent\", \"model\", \"failures\"],\n",
    "        [\"clean\", \"text\", \"better\", \"models\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d2b61-7fcd-495e-b50c-16325d5894d2",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "What Are Stopwords?\n",
    "\n",
    "Stopwords are high-frequency terms that usually carry:\n",
    "\n",
    "- Little semantic value\n",
    "\n",
    "- Low discriminative power\n",
    "\n",
    "Examples: `is, the, and, or`\n",
    "\n",
    "### Load Stopword List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9e9342-ef22-41ef-932d-163f46fc0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2104d-c319-4cf3-9ffd-6fc86ab74ee3",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e753adb7-218e-4cfe-8f36-17f9331ea2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, is, amazing, visit, now]</td>\n",
       "      <td>[amazing, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, is, hard, or, is, it]</td>\n",
       "      <td>[nlp, hard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokens  \\\n",
       "0                  [this, is, amazing, visit, now]   \n",
       "1                      [nlp, is, hard, or, is, it]   \n",
       "2  [tokenization, errors, silent, model, failures]   \n",
       "3                    [clean, text, better, models]   \n",
       "\n",
       "                               tokens_no_stopwords  \n",
       "0                                 [amazing, visit]  \n",
       "1                                      [nlp, hard]  \n",
       "2  [tokenization, errors, silent, model, failures]  \n",
       "3                    [clean, text, better, models]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokens: List[str]) -> List[str]:\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "df[\"tokens_no_stopwords\"] = df[\"tokens\"].apply(remove_stopwords)\n",
    "df[[\"tokens\", \"tokens_no_stopwords\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f4e54-58fb-4b56-a156-496086781bee",
   "metadata": {},
   "source": [
    "# Design Warning\n",
    "\n",
    "Do NOT blindly remove stopwords when:\n",
    "\n",
    "- Sentiment matters (not, never)\n",
    "\n",
    "- Question detection is important\n",
    "\n",
    "- Legal or medical language is involved\n",
    "\n",
    "- Custom stopword lists are often superior.\n",
    "\n",
    "# Stemming (Baseline Approach)\n",
    "What Is Stemming?\n",
    "\n",
    "- Rule-based suffix stripping\n",
    "\n",
    "- Fast but linguistically crude\n",
    "\n",
    "- Can produce non-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28cd8410-b78f-429d-8467-799e6b85a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>tokens_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[amazing, visit]</td>\n",
       "      <td>[amaz, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, hard]</td>\n",
       "      <td>[nlp, hard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "      <td>[token, error, silent, model, failur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_no_stopwords  \\\n",
       "0                                 [amazing, visit]   \n",
       "1                                      [nlp, hard]   \n",
       "2  [tokenization, errors, silent, model, failures]   \n",
       "3                    [clean, text, better, models]   \n",
       "\n",
       "                          tokens_stemmed  \n",
       "0                          [amaz, visit]  \n",
       "1                            [nlp, hard]  \n",
       "2  [token, error, silent, model, failur]  \n",
       "3           [clean, text, better, model]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "df[\"tokens_stemmed\"] = df[\"tokens_no_stopwords\"].apply(stem_tokens)\n",
    "df[[\"tokens_no_stopwords\", \"tokens_stemmed\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8339e8c-5409-49ea-9224-3f67ece46a97",
   "metadata": {},
   "source": [
    "## When to Use Stemming\n",
    "\n",
    "- ✅ Fast baselines\n",
    "- ✅ Search / retrieval\n",
    "- ❌ Interpretability-critical models\n",
    "\n",
    "# Lemmatization (Preferred)\n",
    "What Is Lemmatization?\n",
    "\n",
    "- Dictionary-based normalization\n",
    "\n",
    "- Preserves real words\n",
    "\n",
    "- Requires part-of-speech context (simplified here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b260a883-2c4b-47d6-8854-eb55b540259e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[amazing, visit]</td>\n",
       "      <td>[amazing, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, hard]</td>\n",
       "      <td>[nlp, hard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "      <td>[tokenization, error, silent, model, failure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_no_stopwords  \\\n",
       "0                                 [amazing, visit]   \n",
       "1                                      [nlp, hard]   \n",
       "2  [tokenization, errors, silent, model, failures]   \n",
       "3                    [clean, text, better, models]   \n",
       "\n",
       "                               tokens_lemmatized  \n",
       "0                               [amazing, visit]  \n",
       "1                                    [nlp, hard]  \n",
       "2  [tokenization, error, silent, model, failure]  \n",
       "3                   [clean, text, better, model]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "df[\"tokens_lemmatized\"] = df[\"tokens_no_stopwords\"].apply(lemmatize_tokens)\n",
    "df[[\"tokens_no_stopwords\", \"tokens_lemmatized\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea21c1a-3e5f-40d1-afc4-a8c84726463b",
   "metadata": {},
   "source": [
    "# Stemming vs Lemmatization\n",
    "\n",
    "| Aspect           | Stemming             | Lemmatization |\n",
    "| ---------------- | -------------------- | ------------- |\n",
    "| Speed            | Very fast            | Slower        |\n",
    "| Output           | May be invalid words | Valid words   |\n",
    "| Interpretability | Low                  | High          |\n",
    "| Production NLP   | Rare                 | Preferred     |\n",
    "\n",
    "\n",
    "\n",
    "# Handling Rare or Noisy Tokens\n",
    "\n",
    "Rare tokens often represent:\n",
    "\n",
    "- Typos\n",
    "\n",
    "- OCR errors\n",
    "\n",
    "- One-off identifiers\n",
    "\n",
    "### Simple Length Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b9e5990-5f75-4a06-b510-746503ac290f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "      <th>tokens_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[amazing, visit]</td>\n",
       "      <td>[amazing, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, hard]</td>\n",
       "      <td>[nlp, hard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, error, silent, model, failure]</td>\n",
       "      <td>[tokenization, error, silent, model, failure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokens_lemmatized  \\\n",
       "0                               [amazing, visit]   \n",
       "1                                    [nlp, hard]   \n",
       "2  [tokenization, error, silent, model, failure]   \n",
       "3                   [clean, text, better, model]   \n",
       "\n",
       "                                 tokens_filtered  \n",
       "0                               [amazing, visit]  \n",
       "1                                    [nlp, hard]  \n",
       "2  [tokenization, error, silent, model, failure]  \n",
       "3                   [clean, text, better, model]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_short_tokens(tokens: List[str], min_len: int = 3) -> List[str]:\n",
    "    return [t for t in tokens if len(t) >= min_len]\n",
    "\n",
    "df[\"tokens_filtered\"] = df[\"tokens_lemmatized\"].apply(remove_short_tokens)\n",
    "df[[\"tokens_lemmatized\", \"tokens_filtered\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d8d2e-f3ba-45bf-bfee-271f97a0376f",
   "metadata": {},
   "source": [
    "# Pipeline-Safe Normalization Function\n",
    "\n",
    "All logic must be:\n",
    "\n",
    "- Deterministic\n",
    "\n",
    "- Stateless\n",
    "\n",
    "- Reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1be287be-9a2f-4231-92b6-050ff2fbcc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, is, amazing, visit, now]</td>\n",
       "      <td>[amazing, visit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[nlp, is, hard, or, is, it]</td>\n",
       "      <td>[nlp, hard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tokenization, errors, silent, model, failures]</td>\n",
       "      <td>[tokenization, error, silent, model, failure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[clean, text, better, models]</td>\n",
       "      <td>[clean, text, better, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tokens  \\\n",
       "0                  [this, is, amazing, visit, now]   \n",
       "1                      [nlp, is, hard, or, is, it]   \n",
       "2  [tokenization, errors, silent, model, failures]   \n",
       "3                    [clean, text, better, models]   \n",
       "\n",
       "                               tokens_normalized  \n",
       "0                               [amazing, visit]  \n",
       "1                                    [nlp, hard]  \n",
       "2  [tokenization, error, silent, model, failure]  \n",
       "3                   [clean, text, better, model]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_tokens(tokens: List[str]) -> List[str]:\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "    return tokens\n",
    "\n",
    "df[\"tokens_normalized\"] = df[\"tokens\"].apply(normalize_tokens)\n",
    "df[[\"tokens\", \"tokens_normalized\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda4121-4e94-44ea-a219-313da4611280",
   "metadata": {},
   "source": [
    "# Common Normalization Mistakes\n",
    "\n",
    "- ❌ Removing negations (not, no)\n",
    "- ❌ Mixing stemming and lemmatization\n",
    "- ❌ Applying different stopword sets across splits\n",
    "- ❌ Normalizing after vectorization\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "- Stopwords reduce noise but can remove meaning\n",
    "\n",
    "- Lemmatization is safer than stemming\n",
    "\n",
    "- Normalization choices affect interpretability\n",
    "\n",
    "- Always encapsulate logic in reusable functions\n",
    "\n",
    "# Next Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62576dda-8cf3-4d78-8963-47366aff63a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76408a84-cd58-4869-adf9-3ad837d2bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
