{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd20bd48-23a9-4f4b-bdf7-8c5a00d69254",
   "metadata": {},
   "source": [
    "# Word Embeddings: Word2Vec and GloVe\n",
    "## Objective\n",
    "\n",
    "Introduce dense word-level vector representations that capture semantic similarity, enabling:\n",
    "\n",
    "- Meaning-aware feature engineering\n",
    "\n",
    "- Similarity search and clustering\n",
    "\n",
    "- Stronger document representations than BoW / TF-IDF\n",
    "\n",
    "> This notebook focuses on using pre-trained embeddings, not training them from scratch.\n",
    "\n",
    "## Why Word Embeddings Matter\n",
    "\n",
    "Sparse models treat words as independent symbols.\n",
    "Embeddings instead encode:\n",
    "\n",
    "- Semantic similarity `(king ≈ queen)`\n",
    "\n",
    "- Contextual proximity\n",
    "\n",
    "- Smooth vector spaces\n",
    "\n",
    "They allow models to generalize beyond exact token matches.\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17130b7c-9c30-4520-b49e-f6dd6fce6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a62ce-36cf-436a-884f-2f7ef2daa73c",
   "metadata": {},
   "source": [
    "Note: Pre-trained embedding files (Word2Vec / GloVe) are not committed to the repo due to size.\n",
    "\n",
    "# Loading Pre-Trained Embeddings\n",
    "## Word2Vec (Google News – 300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2949bcc1-2e20-4124-9600-229fc43f28c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example path – adjust to your environment\n",
    "word2vec_path = \"./pre-trained-model/GoogleNews-vectors-negative300.bin\"\n",
    "word2vec_path = \"./pre-trained-model/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "    word2vec_path,\n",
    "    binary=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ea79c-efa3-4b0b-8207-9b126d4a82c9",
   "metadata": {},
   "source": [
    "# GloVe (Converted to Word2Vec Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89e174d3-35b9-4060-8cd1-126994374b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Glove Word2vec at Kaggle.com\n",
    "# Source - https://www.kaggle.com/datasets/serquet/word2vecglove6b300d\n",
    "glove_path = \"./pre-trained-model/word2vec-glove.6B.300d.txt\"\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format(\n",
    "    glove_path,\n",
    "    binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3eba46-9dd5-4e01-aca7-b6b21ee57132",
   "metadata": {},
   "source": [
    "# Vocabulary Coverage Check\n",
    "## Why This Matters\n",
    "\n",
    "Out-of-vocabulary (OOV) words silently degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61c2f0e-e686-44c6-a067-c072c4e140dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_coverage(tokens, model):\n",
    "    covered = [t for t in tokens if t in model]\n",
    "    return len(covered) / len(tokens) if tokens else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b41aa59-d3fd-47d5-8256-12cb0bd2191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec coverage: 0.5\n",
      "GloVe coverage: 0.75\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"clean\", \"model\", \"nlp\", \"foobar\"]\n",
    "\n",
    "print(\"Word2Vec coverage:\", embedding_coverage(tokens, word2vec))\n",
    "print(\"GloVe coverage:\", embedding_coverage(tokens, glove))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a61f6db-068f-42d1-be68-fd042aaf6694",
   "metadata": {},
   "source": [
    "## Word-Level Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "694968bb-695f-4605-a7e1-9a1434d74650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.2745423)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.similarity(\"model\", \"algorithm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12709a5-946c-485d-a40f-ad3a38c22dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('models', 0.744627833366394),\n",
       " ('Model', 0.5217961668968201),\n",
       " ('xenograft_mouse', 0.5016441345214844),\n",
       " ('Brother_Nidal', 0.4816089868545532),\n",
       " ('Smallpox_infection', 0.4770216941833496)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_words = word2vec.most_similar(\"model\", topn=5)\n",
    "similar_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b2de1-4046-482d-b5c2-190f7c111b5f",
   "metadata": {},
   "source": [
    "## Cosine Similarity Between Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94601910-b62a-4357-a19c-0cb4943afb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.644293)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_similarity(word1, word2, model):\n",
    "    return cosine_similarity(\n",
    "        model[word1].reshape(1, -1),\n",
    "        model[word2].reshape(1, -1)\n",
    "    )[0][0]\n",
    "\n",
    "word_similarity(\"good\", \"excellent\", word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac2c4f-d1fb-4177-8193-9cb0335a6e39",
   "metadata": {},
   "source": [
    "# From Word Embeddings to Document Embeddings\n",
    "## Why Aggregation Is Needed\n",
    "\n",
    "Word embeddings are:\n",
    "\n",
    "- Fixed-length\n",
    "\n",
    "- Word-level\n",
    "\n",
    "Models need document-level vectors.\n",
    "\n",
    "## Mean Pooling (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a3662fe-cfa7-49d4-ba1e-547825425cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_embedding(tokens, model):\n",
    "    vectors = [model[t] for t in tokens if t in model]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8c02e-4651-4ca2-bd92-3e6411a68355",
   "metadata": {},
   "source": [
    "## Example Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf0a82c0-b33e-4eb7-af72-63c41e514316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    [\"clean\", \"text\", \"better\", \"model\"],\n",
    "    [\"terrible\", \"results\", \"poor\", \"performance\"]\n",
    "]\n",
    "\n",
    "doc_embeddings = np.vstack([\n",
    "    document_embedding(doc, word2vec)\n",
    "    for doc in docs\n",
    "])\n",
    "\n",
    "doc_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e0b8c4b-e956-407c-9d2a-ca019e4697a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.053162</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.037354</td>\n",
       "      <td>-0.051529</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>0.034302</td>\n",
       "      <td>-0.079468</td>\n",
       "      <td>0.086975</td>\n",
       "      <td>0.063416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.092163</td>\n",
       "      <td>-0.107666</td>\n",
       "      <td>-0.019409</td>\n",
       "      <td>0.021194</td>\n",
       "      <td>0.051018</td>\n",
       "      <td>-0.025660</td>\n",
       "      <td>-0.075684</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.036841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011963</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.064331</td>\n",
       "      <td>0.048645</td>\n",
       "      <td>0.023575</td>\n",
       "      <td>0.085205</td>\n",
       "      <td>-0.038025</td>\n",
       "      <td>-0.137435</td>\n",
       "      <td>0.179443</td>\n",
       "      <td>0.186157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027008</td>\n",
       "      <td>0.201172</td>\n",
       "      <td>-0.199463</td>\n",
       "      <td>-0.038147</td>\n",
       "      <td>-0.064392</td>\n",
       "      <td>0.055603</td>\n",
       "      <td>0.020752</td>\n",
       "      <td>-0.034363</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>-0.050964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.053162  0.065186  0.069672  0.037354 -0.051529  0.057495  0.034302   \n",
       "1  0.011963  0.156250  0.064331  0.048645  0.023575  0.085205 -0.038025   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0 -0.079468  0.086975  0.063416  ... -0.090332  0.092163 -0.107666 -0.019409   \n",
       "1 -0.137435  0.179443  0.186157  ... -0.027008  0.201172 -0.199463 -0.038147   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.021194  0.051018 -0.025660 -0.075684 -0.025513 -0.036841  \n",
       "1 -0.064392  0.055603  0.020752 -0.034363  0.093536 -0.050964  \n",
       "\n",
       "[2 rows x 300 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660cc49-4639-4d57-9b69-eb6b723ce78e",
   "metadata": {},
   "source": [
    "## Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfa8ad72-ebff-4b96-bce6-82d1e5b6e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999999 , 0.38635933],\n",
       "       [0.38635933, 0.9999999 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef98361-ae7a-43b1-997b-f3ebdd61f99d",
   "metadata": {},
   "source": [
    "# TF-IDF Weighted Embedding (Improved)\n",
    "### Why Weighting Helps\n",
    "\n",
    " - Common words dominate mean pooling\n",
    "\n",
    " - TF-IDF emphasizes discriminative terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ed7a7c2-030e-4d4a-9749-3ac1f891aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5b9b73c-207a-462e-8475-c9f13c1149fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\" \".join(doc) for doc in docs]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "tfidf_vocab = tfidf.vocabulary_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bd744f0-52b4-4445-a63a-3cf660adaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighted_embedding(tokens, model, tfidf, vocab):\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in model and token in vocab:\n",
    "            vectors.append(model[token])\n",
    "            weights.append(tfidf.idf_[vocab[token]])\n",
    "    \n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.average(vectors, axis=0, weights=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d93e61-468d-4c64-a2e7-ce54e70d02d9",
   "metadata": {},
   "source": [
    "# Key Limitations of Word Embeddings\n",
    "\n",
    "- Static (no context awareness)\n",
    "\n",
    "- Polysemy not handled (bank)\n",
    "\n",
    "- Require aggregation heuristics\n",
    "\n",
    "- OOV handling is crude\n",
    "\n",
    "# When Word Embeddings Are a Good Choice\n",
    "\n",
    "- `[ok] - ` Small to medium datasets\n",
    "- `[ok] - ` Semantic similarity tasks\n",
    "- `[ok] - ` Clustering / retrieval\n",
    "- `[ok] - ` Feature inputs for classical ML\n",
    "\n",
    "# Common Mistakes\n",
    "\n",
    "- `[cons] - ` Treating word embeddings as sentence embeddings\n",
    "- `[cons] - ` Ignoring OOV rates\n",
    "- `[cons] - ` Training ML models on raw word vectors\n",
    "- `[cons] - ` Forgetting normalization\n",
    "\n",
    "# Key Takeaways\n",
    "\n",
    "- Word embeddings encode semantic similarity\n",
    "\n",
    "- Document embeddings require aggregation\n",
    "\n",
    "- TF-IDF weighting improves signal quality\n",
    "\n",
    "- Word embeddings bridge classical NLP and deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0363527b-c029-4c31-931e-11019d275304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
